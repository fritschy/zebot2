#!/usr/bin/env python3

from handler_lib import handler_exit, get_split_args, get_to, get_from
import ollama
import json
import os
import time
import urllib.request

from ollama_config import *

model = DEFAULT_MODEL
a = get_split_args()

def list_models_with_timeout(host: str, timeout: int) -> dict[str, list[int]]:
    r = urllib.request.urlopen(f'http://{host}:11434/api/tags', timeout=timeout)
    j = json.loads(r.read())
    return j

def check_arg(a: list[str]) -> None:
    if len(a) == 0 or len(' '.join(a).strip()) == 0:
        handler_exit(0, box=1, lines=[f'Was there a question?'], title='error: no question')

# all the models we have
models = []

# host to model
hm = {}
# model to host
mh = {}

# try a couple of hosts
for i in OLLAMA_HOSTS:
    try:
        ms = [x['name'].split(':latest')[0] for x in list_models_with_timeout(i, 1)['models']]
        for m in ms:
            ml = mh.get(m)
            if ml is None:
                mh[m] = []
                ml = mh[m]
            ml.append(i)
        hm[i] = ms
        models += ms
    except:
        continue

models = list(set(models))
models.sort()

if models is None or len(models) == 0:
    handler_exit(0, box=1, lines=['No ollama server available.', 'Try again later or ask my master.'], title='error: no server')

check_arg(a)

while len(a) > 0 and a[0].startswith('-'):
    ma = a[0][1:]
    del a[0]

    if ma in ('h', '-help', 'help'):
        handler_exit(0, box=1, lines=['Usage: !ask [-c|--clear] [-$model] $question', '',
                                      'Valid models are:',
                                      *list(map(lambda x: '  -'+x+('' if x != model else '  (default)'),
                                          models))], title='usage')

    if ma in models:
        model = ma
    else:
        handler_exit(0, box=1, lines=[f'Valid models are {", ".join(map(lambda x: "-"+x, models))}'],
                     title='error: unknown model')

    break

# we fumbled around with args, so check again
check_arg(a)
host = mh[model][0]

r = ollama.Client(host=f'http://{host}:11434').generate(model=model, prompt=CONTEXT_AUGMENTATION.get(model, lambda: '')() + ' '.join(a))
handler_exit(0, lines=list(map(str.strip, r['response'].splitlines())), box=1, wrap_single_lines=1, title=f'response: {host}/{model}')

# vim:set ft=python:
